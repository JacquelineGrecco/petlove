A minha proposta de arquitetura é: Usar DAGs do Airflow para fazer a extração do ERP, VPN e da API, pois assim é possível setar cada job para rodar de acordo com as permissões de cada plataforma. O caso do CRM, eu trataria como mensageria, e por isso iria enviar todos os eventos do CRM para o RabbitMQ para criar essa fila e enviar tudo que ficar pendente nessa fila para o Kinises, e assim, a cada 15 minutos enviar os dados brutos do CRM e também das outras platoformas para um bucket do S3 na AWS. Após isso, eu usaria o EMR com Hadoop para fazer o agrupamento por data e/ou sku (nas bases possíveis) para que todos os arquivos fiquem totalmente agrupados. Após isso, o próximo passo seria fazer o agrupamento por mes/ano para que todos os dados daquele mes/ano fique no mesmo espaço no S3 para facilitar a escrita e também a perfomance. A transformação para parquet, acontece no momento de realizar o primeiro agrupamento para facilitar o restante do processo. Após todas os agrupamentos, isso seria enviado para um segundo bucket no S3 e assim, conectado no Glue, Athena para que as ETLs (em python/scala) que compõe a construção do Data Warehouse possa realizar todo o processo de forma mais rápida, e consequemente os dados transformados seriam colocados em um Redshift (um banco colunar, com uma ótima perfomance para BI). Um ponto válido sobre a minha arquitetura é que pensei em usar a modelagem de fatos e dimensões para que a perfomance do Redshift seja ainda maior. Para finalizar, eu iria usar o Looker como ferramenta de visualização, para propagar o self-service BI dentro da Petlove. Com isso, a área de Data Science tem os dados brutos para fazer os modelos e a area de negocio teria os seus dados transformados para a tomada de decisão.